---
title: "Week 9: Exercises with logistic regression"
output: 
  rmdformats::readthedown:
    code_folding: hide
    code_download: true
    highlight: tango
    number_sections: false
---

```{r}
# Load packages
pacman::p_load(tidyverse, haven, kableExtra, knitr, ggformula, jtools)
```
```

# 1997 Voting data

### Based on:

### Ladd, J.M. and Lenz, G.S. (2009) 'Exploiting a Rare Communication Shift to Document the Persuasive Power of the News Media', American Journal of Political Science, 53(2), pp. 394--410.

```{r}
vote <- read.csv("https://cgmoreh.github.io/SSC7001M/data/newspapers.csv")
```

```{r}
lm_vote <- lm(vote_lab_97 ~ age + male + parent_labour + work_class , data = vote)
jtools::summ(lm_vote, digits=3)
```

```{r}
vote %>%
  count(vote_lab_97) %>%
  mutate(prop = prop.table(n)) 
```

Sometimes it's useful to have a more visual representation of variables. Let's make a bar-graph:

```{r}
vote %>%
  ggplot(aes(x=vote_lab_97, y = ..prop.., group = 1)) + 
  geom_bar() +
  geom_text(stat='count', aes(label=..count..), vjust=-1) + 
  xlab("Vote") +
  ylab("Proportion") +  
  ylim(0,.6) ## to extend the axis to see the label
```

```{r}
vote %>%
  count(parent_labour) %>%
  mutate(prop = prop.table(n)) 
```

```{r}
vote %>%
  count(work_class) %>%
  mutate(prop = prop.table(n)) 
```

## Fitting a logistic model

To fit a logistic regression in R, we will use the glm function, which stands for Generalized Linear Model. Within this function - as with the 'lm' function we used for linear models - we write the dependent variable, followed by \~, and then the independent variables separated by +'s. When the family is specified as binomial, R defaults to fitting a logit model.

```{r}
logit_vote <- glm(vote_lab_97 ~ age + male + parent_labour + work_class, family = binomial, data = vote)
jtools::summ(logit_vote, digits=3)
```

## Odds Ratios

The coefficients returned by our logit model are difficult to interpret intuitively, and hence it is common to report odds ratios instead. An odds ratio less than one means that an increase in x leads to a decrease in the odds that y=1. An odds ratio greater than one means that an increase in x leads to an increase in the odds that y=1. In general, the percent change in the odds given a one-unit change in the predictor can be determined as:

$$
Percent Chance in Odds = 100(OR-1)
$$

We can request 'exponentiated coefficints' (odds ratios) using the jtools summary function (or other packages, such as the tidy() function in the broom package).

```{r}
jtools::summ(logit_vote, digits=3, exp = TRUE)
```

For example, the odds of voting Labour are 100\*(3.163-1)= `r 100*(3.163-1)`% higher for those whose parents were Labour voters too. Similarly, the odds are 100\*(2.834-1)=`r 100*(2.834-1)`% higher for those who self-identify as working-class. However, each increase in age by one year leads to a 100\*(.987-1)=`r 100*(.987-1)`% decrease in the odds of voting Labour. 

We can also visualise the regression model results, which is often a good way of describing our results:

```{r}
jtools::plot_summs(logit_vote, exp = TRUE)
```

## Probabilities

Odds ratios are commonly reported, but they are still somewhat difficult to intuit given that an odds ratio requires four separate probabilities. Itâ€™s much easier to think directly in terms of probabilities. However, due to the nonlinearity of the model, it is not possible to talk about a one-unit change in an independent having a constant effect on the probability. Instead, predicted probabilities require us to also take into account the other variables in the model.

One way of obtaining so-called marginal probabilities - probabilities 

```{r}
# install.packages("margins")
library(margins)
summary(margins(logit_vote))
```

We can interpret these results are change in probabilities while also considering the other variables in the model.

