---
title: "Data manipulation"
author: "C Moreh"
output:
  rmdformats::readthedown:
      code_download: false
      number_sections: false
      highlight_downlit: true
 
# distill::distill_article:
  #   toc: true
---

```{r knitr options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp = 0.618, warning=FALSE, comment=NA, prompt=FALSE, message = F, strip.white = F)
```

```{css echo=FALSE}
body {
  counter-reset: counter-rchunks;
}

div.main-container {
  padding-left: 5em;
}

pre.r {
  counter-increment: counter-rchunks;
  position: relative;
  overflow: visible;
}

pre.r::before {
  content: '[Code block 'counter(counter-rchunks)']';
  display: inline-block;
  position: absolute;
  left: 0em;
  top: 0em;
  color: rgb(48, 63, 159);
}
```

Working with real-life data requires a lot of pre-processing before we are able to carry out out main analyses. This pre-processing can involve importing data from various storage formats, applying and changing overall characteristics, filtering and ordering rows (cases), computing, adding and renaming columns (variables), recoding variable values, and computing summary statistics for variables of interest to determine any need to recode variables. These are the main data manipulation techniques that this document will cover. While the procedures we learn here are the ones you will probably require for your own small-scale data analysis projects, some data manipulation requirements are specific to given data sets and they may require some techniques not covered here. Should that be the case, get in touch at [c.moreh\@yorksj.ac.uk](mailto:c.moreh@yorksj.ac.uk?subject=SSC7001M coding help) and describe the problem you need to solve.

Data manipulation tasks can be carried out in plain ***R*** language or by using specially written packages (libraries). The former can often seem less intuitive and reader-friendly, so we will use mainly the popular **`dplyr`** R package, which contains important R functions to carry out easily your data manipulation. **`dplyr`** is part of the wider universe of packages called **`tidyverse`**, whose main aim is to implement ***R*** functions using a more intuitive syntax.

The `dplyr` functions are primarily targeted at data tables, whereas your main challenge is likely to be manipulating variables (i.e. columns in data-tables). You will also likely have to analyise datasets that had been created and made available in a format specific to other statistical software (such as ***Stata*** or ***SPSS***). For this purpose, a suite of packages written by [Daniel Lüdecke](https://github.com/strengejacke) are extremely useful, and you are likely to benefit from using function from the `sjlabelled`, `sjmisc` and `sjPlot` packages. All of these packages integrate well with the `tidyverse` workflow.

The expectation is that you have already read the [R for Data Science](https://r4ds.had.co.nz/r-markdown.html) book and completed the [R for Social Scientists](https://datacarpentry.org/r-socialsci/) online lessons. The content below is but a reminder of some of the material available there and an extension based on specific issues that you are more likely to encounter in your analysis. 

# About the `Tidyverse`

## Data frames and 'tibbles'

The `Tidyverse` is built around the basic concept that data in a table should have one observation per row, one variable per column, and only one value per cell. Once data is in this 'tidy' format, it can be transformed, visualized and modelled for an analysis.

When using functions in the `Tidyverse` ecosystem, most data is returned as a `tibble` object. `Tibbles` are very similar to the `data.frames` (which are the basic types of object storing datasets in base ***R***) and it is perfectly fine to use `Tidyverse` functions on a `data.frame` object. Just be aware that in most cases, the `Tidyverse` function will transform your data into a `tibble.` If you are unobservant, you won't even notice a difference. However, there are a few differences between the two data types, most of which are just designed to make your life easier. For more info, check [R for Data Science](https://r4ds.had.co.nz/tibbles.html) and [R for Social Scientists](https://datacarpentry.org/r-socialsci/02-starting-with-data/index.html)

## Selected `dplyr` functions

The `dplyr` package is designed to make it easier to manipulate flat (2-D) data (i.e. the type of datasets we are most likely to use, which are laid out as in a standard spreadsheet, with rows referring to cases (observations; respondents) and columns referring to variables. `dplyr` provides simple "verbs", functions that correspond to the most common data manipulation tasks, to help you translate your thoughts into code. Here are some of the most common functions in `dplyr`:

  * `filter()` chooses rows based on column values.
  * `arrange()` changes the order of the rows.
  * `select()` changes whether or not a column is included.
  * `rename()` changes the name of columns.
  * `mutate()`/`transmute()` changes the values of columns and creates new columns (variables)
  * `summarise()` compute statistical summaries (e.g., computing the mean or the sum)
  * `group_by()` group data into rows with the same values
  * `ungroup()` remove grouping information from data frame.
  * `distinct()` remove duplicate rows. 

All these functions work similarly as follow:

- The first argument is a data frame/tibble 
- The subsequent arguments are comma separated list of unquoted variable names and the specification of what you want to do
- The result is a new data frame

For more info, check [R for Social Scientists](https://datacarpentry.org/r-socialsci/03-dplyr-tidyr/index.html)


## The forward-pipe (`%>%`) workflow

All of the `dplyr` functions take a data frame or tibble as the first argument. Rather than forcing the user to either save intermediate objects or nest functions, `dplyr` provides the forward-pipe operator `%>%` from the  `magrittr` package. This operator allows us to combine multiple operations into a single sequential chain of actions.

Let’s start with a hypothetical example. Say you would like to perform a sequence of operations on data frame `x` using hypothetical functions `f()`, `g()`, and `h()`:

1.  Take x *then*
2.  Use x as an input to a function f() *then*
3.  Use the output of f(x) as an input to a function g() *then*
4.  Use the output of g(f(x)) as an input to a function h()

One way to achieve this sequence of operations is by using nesting parentheses as follows:

```
h(g(f(x)))
```

This code isn’t so hard to read since we are applying only three functions: `f()`, then `g()`, then `h()` and each of the functions is short in its name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively harder to read as the number of functions applied in your sequence increases and the arguments in each function increase as well. This is where the pipe operator `%>%` comes in handy. `%>%` takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read `%>%` as “then” or “and then.” For example, you can obtain the same output as the hypothetical sequence of functions as follows:

```
x %>% 
  f() %>% 
  g() %>% 
  h()
```

You would read this sequence as:

1. Take x *then*
2. Use this output as the input to the next function f() *then*
3. Use this output as the input to the next function g() *then*
4. Use this output as the input to the next function h()

So while both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. Instead of typing out the three strange characters of the operator, one can use the keyboard shortcut *Ctrl + Shift + M* (Windows) or *Cmd + Shift + M* (MacOS) to paste the operator.

Note that since `R 4.1.0` there is also a [native pipe operator](https://www.r-bloggers.com/2021/05/the-new-r-pipe/) in ***R*** (|>), and in RStudio one can set the shortcut to paste the new pipe operator instead.

The `magrittr` package provides some other piping functions too. One of the most useful ones is the so-called assignment pipe operator `%<>%` in which the object mentioned to its left serves both as the initial value and as target (read more [here](https://rdrr.io/cran/magrittr/man/compound.html)). It is a useful tool because often our aim with a piping sequence is to update/overwrite an original object with the result. In other words, our aim often is:

```
x <- x %>% 
        f() %>% 
        g() %>% 
        h()
```
 0. <span style="color: red;">Overwrite x with the output of the following commands:</span> 
<br>
 1. Take x *then*
 2. Use this output as the input to the next function f() *then*
 3. Use this output as the input to the next function g() *then*
 4. Use this output as the input to the next function h()

Using the assignment operator, we can save on typing by using:

```
x %<>% 
  f() %>% 
  g() %>% 
  h()
```
Note, however, that the assignment operator is not ported with the `tidyverse` packages, so if you want to use it, you will need to install/load the `magrittr` package separately. To avoid any confusion, we won't use the `%<>%` operator in this tutorial. Also, it is often safer not to overwrite an existing object (e.g. dataset), but to create a new one, then delete the ones you no longer need:
```
x2 <- x %>% 
        f() %>% 
        g() %>% 
        h()
```

# Installing and loading user-written packages

Instead of programming your own functions in the ***R*** language, you can rely on functions written by other people and bundled within a package that performs some set task. There are a large number of reliable, tested and oft-used packages containing functions that are particularly useful for social scientists.  

Some particularly useful packages:
  - the `tidyverse` bundle of packages, which includes the `dplyr` package (for data manipulation) and additional R packages for reading in (`readr`), transforming (`tidyr`) and visualizing (`ggplot2`) datasets. It also ports the `%>%` so-called pipe operator that helps write more human-readable code
  - to import datasets in non-native formats and to manage attached labels (a concept familiar from other statistical packages but foreign to ***R***), load the `sjlabelled` package (an alternative to `haven` and `labelled`, which work in a similar way but provide less functionality) 
  - the `summarytools` package can be useful for making simple summary tables and crosstabulations
  - the `sjmisc` package contains very useful functions for undertaking data transformations on labelled variables (recoding, grouping, missing values, etc); also has some useful tabulation functions
  - the `sjPlot` package contains functions for graphing and tabulating results from regression models
  - the `mosaic` and `ggformula` packages provide functions to write data visualisation commands using "formula syntax", which is particularly useful for introductory-level teaching/learning as it unifies data description and statistical modelling around a shared logical structure (read about it [here](http://www.mosaic-web.org/ggformula/articles/pkgdown/ggformula-blog.html) and [here](http://www.mosaic-web.org/mosaic/articles/web-only/LessVolume-MoreCreativity.html))

Packages are often available from the *Comprehensive R Archive Network* (CRAN) or private repositories such as *Bioconductor*, *GitHub* etc. Packages made available on CRAN can be installed using the command `install.packages("packagename")` and then loaded using the command `library(packagename)`.

There are also convenience tools - e.g. the `pacman` package - that make it easier to load several packages at once, while at the same time downloading the package if it has not yet been downloaded.

Let's install/load a number of packages:

```{r message=FALSE, warning=FALSE}

# Install 'pacman' if not yet installed:

if (!require("pacman")) install.packages("pacman") 

# Load/install other packages using 'pacman':

pacman::p_load(
  tidyverse,    # general data management tools ('dplyr', etc.)
  mosaic,       # formula-type syntax for descriptive statistics
  ggformula,    # ggplot2 powered graphing using 'mosaic' formula-syntax
  lme4,         # multilevel modelling
  summarytools, # summary statistics tables
  sjlabelled,   # data import from other software (alternative to 'haven') and labels management
  sjmisc,       # data transformation on variables (recoding,grouping, missing values, etc)
  sjPlot,       # Graphing and tabulation tools for regression model results
  jtools,        # Graphing and tabulation tools for regression model results
  modelsummary
  )
```

You can check the suite of packages that are loaded when you load the `Tidyverse` library using the following command:

```{r}

tidyverse_packages()
```

Alternatively, if you only plan on using one of the packages included in the `Tidyverse`, you can load only that one.

Once a package is loaded, you can use any function from it. If you attempt to use a function without having loaded the package (e.g. `no.such.function(do.this.and.that)`), you will get an error message such as: 
```{r echo=FALSE, error=TRUE}
no.such.function(do.this.and.that)
```

Different packages can contain functions with the same name but which perform very different tasks. Functions are made available in the reverse order in which a package was loaded; so, if you first load package `a`, then package  `b`, and both contain a function called `funk()`, then `funk()` from package `a` will be 'masked' by `funk()` from package `b` and the latter will be applied when calling the function. You can explicitly provide a package name when you call a function, using the double colon operator `::` (e.g. `a::funk()`). Using `::` is also useful when you have a package installed but you don't want to load into the computer memory its entire library of functions, because you only plan on using one function only a few times in a session. This saves computer memory. Also, it's a good idea to use `::` often at the beginning of your **R** learning journey to keep a note on where the different functions come from. This also makes your code easier to decipher for others who may read your analysis scripts. You'll see `::` occasionally in this document.

# Importing datasets from other software packages

The data you will want to use is likely to come in a format used by another statistical package or data management tool (***SPSS***, ***Stata***, ***Excel***) or in raw *Comma Separated Values (CSV)* or *tab separated values (TSV)* files. We have practised importing ***Stata*** (.dta) and ***SPSS*** (.sav) data files in particular, as it is the format in which much of the large survey data made available to researchers in the *UK Data Service* is stored. The *.csv* format is also very common and probably the most flexible one to use across different platform and openable in Microsoft Excel and equivalents. In can be imported using the base ***R*** function `read.csv()` function, which is a bit slow but okay for smaller datasets. For larger datasets the `read_csv()` function from the `readr` package (in `tidyverse`) is available. See also [R for Social Scientists](https://datacarpentry.org/r-socialsci/02-starting-with-data/index.html).

To practice data wrangling on real data, we can use some datasets available from the course's (not quite developed as of yet) webpage: https://cgmoreh.github.io/SSC7001M/data/data-documentation. These data can be imported from the web location: https<!-- -->://cgmoreh.github.io/SSC7001M/data/<span style="color: red;">File name</span>. The files **eb89.1**, **ess9** and **evs5** are cross-sectional datasets for single waves/rounds of the *Eurobarometer*, *European Social Survey* and *European Values Study*, respectively, in the original formats made publicly available by the survey teams. A such, these datasets are the most useful for practising data cleaning, as they will necessarily be rather messy in various ways.

Let's use the European Values Study, Wave 5 (2017-2020) dataset and import the **evs5** file from the original ***Stata*** format using the `read_stata()` function from the `sjlabelled` package (an alternative is `read_dta()`/`read_stata()` from the `haven` package):

```{r}
evs <- sjlabelled::read_stata("https://cgmoreh.github.io/SSC7001M/data/evs5.dta")
```

If a dataset you want to import is in another format, you can use the appropriate function to `read_*format type*`. You can check a list of the functions available in a package using the `ls()` listing command; for example:

```{r}
# List all functions in the 'sjlabelled' package that contain the word "read":

ls("package:sjlabelled", pattern = "read") 
```

Once a dataset is loaded, you can have a quick look at some of its characteristics (which you can also check manually in the Environment panel of RStudio or by opening the dataset in the Viewer). 

```{r}
# Number of rows (as first element), and the number of columns (as the second element); the 'dimensions' of the object:
dim(evs)

# Or separately, the number of rows (cases, observations):
nrow(evs)

# And number of columns (variables):
ncol(evs)
```

Wee see that the loaded **evs** dataset contains `r format(nrow(evs), big.mark=",")` observations and `r ncol(evs)` variables. That's a very large dataset that would benefit from some data reduction and transformation before being used for the actual statistical analysis we want to undertake.

# Data transformation workflow

In each data analysis project we encounter specific data manipulation challenges. Some data transformation tasks are very common in a majority of projects using large survey-based social science data, and we can think about performing these tasks in terms of a workflow, a logical step-by-step order in which to manipulate the data to end up with a dataset that can easily be submitted to statistical analysis. Let's follow such a hypothetical workflow to cover some of these most common requirements. Our hypothetical data analysis aim will be to estimate and report on a model predicting <tt>*trust in strangers*</tt> from <tt>*age*</tt>, <tt>*sex*</tt>, <tt>*religiosity*</tt>, and <tt>*political ideology*</tt> among the UK population.

Let's see what challenges we face and address them step-by-step.


## There are too many variables (columns) in the dataset

Sometimes (often? always?) it's useful to select only a few variables that we are interested in using for analysis. With externally imported data we probably have access to the survey documentation and a data codebook (e.g. https://europeanvaluesstudy.eu/methodology-data-documentation/survey-2017/pre-release-evs-2017/documentation-survey-2017/). We can identify variables of interest in that documentation and select them either when importing the data or as a later step, using the `select()` function from `dplyr`. We can either mention the location number of the variables we want to keep or, more usefully, the names of the variables. We separate the variable names using a comma, or we can specify a range of variables using "**:**", as in the example below, where we keep all the variables between <tt>v35</tt> and <tt>v37</tt> from the **evs** dataset as well as other variables that are useful for addressing our hypothetical data analysis aim^[Note that in a real statistical analysis scenario you would make sure to import any sampling design variables that are required to apply the necessary weighting to your analysis; for example, [read here about weighting data in the European Values Study (2017)](https://nbn-resolving.org/urn:nbn:de:0168-ssoar-70113-4)]: 

```{r eval=FALSE, include=T}
evs %>% select("country", "v31", "v33", "v35":"v37", "v54", "v55", "v102", "v225", "age")
```

There are also several special functions that can be used inside `select()`, such as `starts_with()`, `ends_with()`, `contains()`, `matches()`, `one_of()`, etc. (read more [here](https://rdrr.io/cran/dplyr/man/select.html)).

To actually collapse the loaded dataset to the selected variables, we have to assign (with "<-") an object name. If we assign the same name as an already loaded dataset object, the data will be overwritten:

```{r}
# We can save the reduced dataset to a new data frame object called 'evs.slim'(because we have reduced the number of its columns)
evs.slim <- evs %>% select("country", "v31", "v33", "v35":"v37", "v54", "v55", "v102", "v225", "age")

# Or we can overwrite the already existing data frame object called 'evs'
evs <- evs %>% select("country", "v31", "v33", "v35":"v37", "v54", "v55", "v102", "v225", "age")
```

## I don't need all the observations (rows)

We can also select a subset of cases/observations for analysis. We may be interested in analysing only observations that satisfy certain criteria. For example, our hypothetical data analysis aim relates only to observations from the United Kingdom, whereas the **evs** dataset contains observations from many different countries. The `filter()` function (from `dplyr`) can be used to filter rows that meet some logical criteria. For setting our criteria we can use ***R***'s logical comparisons and operators:

1. Logical comparisons:
 - **<**   for less than
 - **>**   for greater than
 - **<=**   for less than or equal to
 - **>=**   for greater than or equal to
 - **==**   for equal to each other
 - **!=**   for not equal to each other
 - **%in%**   group membership; for example, “age %in% c(16, 17)” specifies those aged 16 or 17 only.
 - **is.na()**   is NA (i.e. missing)
 - **!is.na()**   is not NA (i.e. missing)

2. Logical operators:
 - **|**   means **or** (e.g. age == 16|17 ("age %in% c(16, 17)" is a shortcut with the same meaning)
 - **&**   means **and** (e.g. sex == “female” & age > 25)

One frequent mistake is to use **=** instead of **==** when testing for equality. When you are testing for equality, you should always use **==** (not **=**).

For example let's select from the already slimmed dataset only cases that come from respondents in the United Kingdom. The <tt>country</tt> variable codes the country of data collection. We can get a list of variable labels using `get_labels()` from `sjlabelled`, which has the option to also request that labels be prefixed with their values, which is useful to see for recoding purposes (read more about the `sjlabelled` package and its functions [here](https://rdrr.io/cran/sjlabelled/)): 

```{r}

# Get a list of variable labels with values prefixed (i.e. ="p" or ="as.prefix")

sjlabelled::get_labels(evs.slim$country, values ="as.prefix")

# Or, equivalently, first selecting the variable(s); this allows listing several variables within the select() function
# evs %>% select(country) %>% 
#  sjlabelled::get_labels(values = "as.prefix")
```

We can see in the list that the country label is 'Great Britain' and it is coded as 826, so we can filter the data using the following command (assigning the reduced dataset to a new object in this case, but we could also overwrite the existing one:

```{r}

# Notice the use of the double-equal == operator to compare equality (select if country code is equal to 826)

evs.slim.gb <- evs.slim %>% filter(country==826)
```

We could also make the selection using the variable label instead of the numeric code, but because the 'country' variable is currently stored as a labelled **numeric** vector, that wouldn't work straight away because operations are performed on the *values* rather than the *labels* (read all there is to know about vector types in ***R*** in [R for Data Science](https://r4ds.had.co.nz/vectors.html?q=typeof#vector-basics)). Instead, we need to convert the variable to another type first, such as **character** (i.e. text) or labelled **factor** (i.e. categorical) variable (the `sjlabelled` package has functions to replace values of a variable with their associated value labels; read more about [`as_factor()`](https://rdrr.io/cran/sjlabelled/man/as_factor.html) and [`as_label()`](https://rdrr.io/cran/sjlabelled/man/as_label.html). Setting the appropriate variable types for statistical analysis is a separate and complex data wrangling step and we'll loot at it in more detail below. But it's often useful to constrain variables to a given type as part of other data manipulation tasks; for example, here we can constrain the <tt>country</tt> variable to be of type **character**/**label** for the purpose of `filter()`ing, so that we can refer to its levels by their label, without actually changing its type in the dataset:

```{r}

evs.slim.gb <- evs.slim %>% filter(as_character(country)=="Great Britain")

# Or, alternatively: 
# ... filter(as_label(country)=="Great Britain")
```

To combine more than one item, such as selecting not one but several countries (e.g. Great Britain and Germany), we can use the `%in%` operator and the `c(...)` function:

```{r}

# Select if country label/name is equal to one of the labels/names listed inside c(...)

evs.slim.gb_de <- evs.slim %>% filter(as_character(country) %in% c("Great Britain", "Germany"))
```

The 'filter' command is highly flexible and also allows to combine multiple criteria if needed, say only British respondents aged between 25 and 35:

```{r}

evs.slim.gb.25_35 <- evs.slim %>% 
  filter(as_character(country) == "Great Britain",
         age >= 25 & age <= 35
         )
```

## I don't need all the objects in my Environment

Saving different versions of the transformed **evs** dataset under different names in the working memory is useful for keeping track of intermediary transformations and cross-checking the changes we have made. But after a while the Environment becomes cluttered with objects that we no longer need. We can remove redundant objects from the Environment using the `rm()`/`remove()` function. Within the function we can list the objects we want to remove (e.g. `rm(evs.slim.gb_de, evs.slim)`; or we can remove all objects using `rm(list = ls())` (here the `ls()` function writes a list of all the objects in the Environment, then `rm()` is applied to that list); or we can remove all objects apart from one/few selected ones that we keep:

```{r}

# Keep only the 'evs.slim.gb' data frame object

rm(list = setdiff(ls(), "evs.slim.gb"))
```

## I want to make basic descriptive summaries of my data

Data-frame summaries are great tools for getting a summary overview of a whole dataset. Apart from the `generate_dictionary()` function from the `labelled` package (which we have used before), the `dfSummary()` function from `summarytools` is another (probably nicer and more universally applicable) simple tool for generating a so-called "data dictionary". The simplest way to use it is to open it in the output viewer using the `view()` function:

```{r eval=FALSE}

evs.slim.gb %>% summarytools::dfSummary() %>% view()

# or equivalently: 
# view(dfSummary(evs.slim.gb))
```

The resulting 'data frame summary' will look something like this:
```{r echo=FALSE, results='asis'}
evs.slim.gb %>% dfSummary(valid.col = FALSE, graph.magnif = .7, plain.ascii = FALSE, style = "grid", tmp.img.dir = "/tmp", headings = F, display.labels = F) %>% 
  print(max.tbl.height = 300)
```

```{r eval=FALSE, include=FALSE}

## A similar function is sjPlot::view_df() , which is more focused on labelled data, so it's not ideal for data overviews because it doesn't show statistics for numeric variables, just primarily as a codebook of labels and values

evs.slim.gb %>% as_character(country) %>% 
  sjPlot::view_df(show.string.values = T, show.frq = T, show.prc = T)
```

```{r eval=FALSE, include=FALSE}
## Modelsummary also has a very similar data-frame tabulation function
evs.slim.gb %>% modelsummary::datasummary_skim()
```
```{r eval=F, include=F}
modelsummary::datasummary(All(evs.slim.gb) ~ (Mean + SD)*Arguments(fmt = "%.2f", na.rm = TRUE) + Median + Min + Max + NUnique + N, align = "lccccccc", fmt = '%g', data = evs.slim.gb)
```


This output can then be opened in a separate web browser window and saved as an HTML file (or saving can be done automatically by replacing the `view()` command with `print( file = "somename.html")`). This is a good way of getting a detailed overview of an entire dataset and identify where data transformations are necessary. It is also a nice way of producing a codebook for an already cleansed dataset.

The `sjmisc` package also has a function called `descr()` for summarising datasets, which is particularly useful if you have labelled data imported from other software. By default, without any additional settings, a call to `sjmisc::descr()` displays a wide variety of summary statistics (see the [function description](https://rdrr.io/cran/sjmisc/man/descr.html)). Using the 'show' argument, we can select only those that we are interested in, for example:

```{r include=FALSE}
library(kableExtra)
```

```{r eval=FALSE, include=T}

# Useful variables were selected from those available in the function
# 'Character' type variables are not included in tables in this function, so if it is appropriate for your purposes, any 'character' variables may need coercing to 'factor' or 'label' first for inclusion

evs.slim.gb %>% sjmisc::descr(show = c("type", "label", "range", "md", "NA.prc"))
```
```{r echo=FALSE}
evs.slim.gb %>% sjmisc::descr(show = c("type", "label", "range", "md", "NA.prc")) %>% kable()

```

<br>

From these tables we find, for instance, that there are no missing data points, but there are many negative values in each variable that probably need to be set as missing values (NAs), that the variable type probably need changing for categorical variables, and that variables would benefit from some more intuitive names.

The `count()` and `summarise()` functions (from `dplyr`) come handy for quick tabulations of single variables. For example:

```{r}

evs.slim.gb %>% count(v31)
```
The same frequency table can be produced with a combination of `group_by()` and `summarise()`:

```{r}

evs.slim.gb %>% group_by(v31) %>% summarise(n = n())
```

This latter option is a bit of an overkill compared to `count()`, but has the advantage that it is very flexible and can be expanded to achieve complex summaries by grouping variables, as we'll see later.

In either case, to see value labels instead of numeric values, we could coerce the variable type to be treated for the purpose of this operation as a labelled 'factor' variable:

```{r, eval=F}

evs.slim.gb %>% count( as_label(v31) )

# or equivalently:
# evs.slim.gb %>% as_label(v31) %>% count(v31)
```
```{r echo=FALSE}
evs.slim.gb %>% count( as_label(v31) )

```

Function `frq()` (from `sjmisc::`) produces more complex frequency tables that handle labelled variables better. For example, this is a frequency table of the same variable <tt>v31</tt>:

```{r eval=FALSE, include=TRUE}

evs.slim.gb %>% frq(v31)
```

```{r echo=FALSE}
evs.slim.gb %>% frq(v31) %>% kable()
```

We see, for instance, that levels (categories) coded with negative values (-1 to -10) can indeed be excluded from the analysis by setting them to NA (missing). Recoding variables involves a lot of subjective decision-making; for example, in some contexts it may make sense to include "don't know" answers into the analysis because *not knowing* something may have a substantive interpretation relevant to the research question. In the case above, however, the small number of "not knowers" can safely be excluded from the analysis. We'll look at how to recode variables in a moment, but let's move from general to specific and first address the issue of having very uninformative variable names.

## I need to rename my variables

It's always a good idea to rename the variables you want to use to names that are more intuitive to you. Often, even when variables have apparently more human-readable names, they may not accurately reflect what the variable entails.

There are several convenient options for renaming variables (columns), such as the `rename()` and `rename_with()` functions in `dplyr` or the `var_rename()`/`rename_variables()` function in `sjmisc`. The main difference is that in case of `rename()` the command is *new variable name* = *old variable name*, whereas in `var_rename()` it is the other way around, *old* = *new*.

Let's rename the **v31** variable we tabulated earlier (labelled: "people can be trusted/can't be too careful (Q7)" ) to **ppl.trust.yn** (to mind mind some shorthand for "people can be trusted, yes or no"). The new name will be longer to type, so it's a trade-off between brevity and clarity. Unless you are very intimately familiar with a survey and it's variable naming conventions, I recommend erring on the side of clarity:

```{r}

evs.slim.gb <- evs.slim.gb %>%
  rename(ppl.trust.yn = v31)

# Or:
# ... %>% sjmisc::var_rename(v31 = ppl.trust.yn)
```

We can rename several variables in one go. For example, let's rename **<tt>v54</tt>** to **<tt>rel.now</tt>**, **<tt>v55</tt>** to **<tt>rel.as.child</tt>**, **<tt>v102</tt>** to **<tt>pol.lr</tt>**, and **<tt>v225</tt>** to **<tt>sex</tt>**:

```{r attr.source='.numberLines'}

# Break down command chains into separate lines to make complex code easier to read and troubleshoot

evs.slim.gb <- evs.slim.gb %>% 
  rename(
    rel.now = v54,
    rel.as.child = v55,
    pol.lr = v102,
    sex = v225
    )
```

We now have more intuitive variable names, but as we transform/recode our variables it may come useful to rename some of them again later to better reflect their actual meaning. Also, note that we haven't renamed all the variables - <tt>v33</tt>, <tt>v35</tt>, <tt>v36</tt> and <tt>v37</tt> remain unchanged - and that's for a reason, as we'll see in a moment.

## I need to assign missing (NA) values

We've seen in the tables above that negative values need recoding to missing values, so that they are not included in our statistical analyses. Missing values in ***R*** are denoted with *NA*. Let's recode the variable we have just renamed to **<tt>ppl.trust.yn</tt>**, and which we have tabulated earlier [Code bloc 21]. The `set_na()` function from `sjlabelled` is very useful for this purpose:

```{r}

# Setting 'as.tag = TRUE' let's us store the labels of the missing values, just in case we'll need them again
 
evs.slim.gb <- evs.slim.gb %>% set_na(ppl.trust.yn, na = c(-1:-10))
```

The result is:
```{r eval=FALSE}

evs.slim.gb %>% frq(ppl.trust.yn)
```
```{r echo=FALSE}
evs.slim.gb %>% frq(ppl.trust.yn) %>% kable()
```

Instead of recoding variables individually, we can recode all the negative values in a dataset to missing:

```{r}

## recode all negative values to missing in the entire dataset, keeping labels as tags:

evs.slim.gb <- evs.slim.gb %>% set_na(na = c(-1:-99))
```

## I need to recode variables

Recoding variables can cover many practical needs. Here, let's cover four very common requirements that often emerge in the manipulation of survey-based social science data: (1) reordering the values of ordinal variables; (2) collapsing categories; (3) shifting the range of values; and (4) recoding numeric scales into equal-rnaged groups.

In order to maintain and manage value and variable labels, we will use some functions from the `{sjmisc}` package, although similar operations can be done using `{dplyr}` and base ***R*** operations.   

### (1) reordering the values of ordinal variables

We can very easily reverse the value order in ordered variables using the `rec()` function. In our data, we have several variables that fall into this category (<tt>v33</tt> to <tt>v37</tt>, <tt>rel.now</tt> and <tt>rel.as.child</tt>). Additionally, we can also reverse the order of values of binary variables (e.g. <tt>ppl.trust.yn</tt>). Before actually altering our data, we can check what the results will be:  

```{r}
evs.slim.gb %>% 
  sjmisc::rec(v33, rec = "rev") %>% frq(v33, v33_r)
```
```{r echo=FALSE}
evs.slim.gb %>% 
  sjmisc::rec(v33, rec = "rev") %>%  frq(v33, v33_r) %>% kable()
```

```{r, eval=FALSE}
evs.slim.gb %>% 
  sjmisc::rec(rel.now, rec = "rev") %>%  frq(rel.now, rel.now_r)
```
```{r echo=FALSE}
evs.slim.gb %>% 
  sjmisc::rec(rel.now, rec = "rev") %>%  frq(rel.now, rel.now_r) %>% kable()
```

Once we know that the results are what we wish, we can reverse all variables at once, and setting the 'suffix' to empty ("") allows to overwrite the existing variables (by default, new variables suffixed by "_r" would be created, which is often the safer option; we can also set our own suffixes):

```{r}
evs.slim.gb <- evs.slim.gb %>% 
  sjmisc::rec(ppl.trust.yn, v33:v37, rel.now, rel.as.child, rec = "rev", suffix = "")
```

### (2) collapsing categories

The `rec()` function from `{sjmisc}` allows specifying in detail how values should be recoded. These are many options apropriate for different purposes, so [read more about the function here](https://rdrr.io/cran/sjmisc/man/rec.html). In this example, let's recode the two 'religiosity' variables so that we collapse their 7 categories to three: rarely (1, 2, 3), moderately (4, 5), often (6, 7):

```{r}
evs.slim.gb %>% 
  sjmisc::rec(rel.now, rel.as.child, rec = "1:3=1; 4,5=2; 6:max=3") %>%  frq(rel.now, rel.now_r)
```



```{r eval=FALSE, include=FALSE}
evs.slim.gb <- evs.slim.gb %>% mutate(no_trust2 = recode(no_trust,
                                         `1`=0, 
                                         `2`=1))
```

But we need to be careful as we then lose the labels:
```{r eval=FALSE, include=FALSE}
get_labels(evs.slim.gb$no_trust2, values = "p")
```

We could recode to character strings instead, but again we need careful because character variable values are then ordered alphabetically:

```{r eval=FALSE, include=FALSE}
evs.slim.gb <- evs.slim.gb %>% mutate(no_trust3 = recode(no_trust,
                                         `1`="trust", 
                                         `2`="no trust"))
```

Recoding several variables by collapsing categories:

```{r eval=FALSE, include=FALSE}
evs.slim.gb <- evs.slim.gb %>% mutate_at(c("v32", "v34"), funs(recode(., `1`=1, `2`=1, `3`=0, `4`=0)))
```


### (3) shifting the range of values

Recode binary variables to be coded 0 and 1, keeping value labels:

```{r}

# The setting 'lowest =" is by default set to 0 so can be excluded

evs.slim.gb %>% sjmisc::recode_to(ppl.trust.yn, sex, suffix = "", lowest = 0) %>% frq(ppl.trust.yn, sex)
```







## Computing new variables

```{r eval=FALSE, include=FALSE}
evs.slim.gb <- evs.slim.gb %>% mutate(computed_v32_34 = v32 + v33 + v34)
```  


## Summary statistics by group

We can produce some useful summary statistics by goups of another variable. For example, let's look at the average value of the variable 'no_trust' (i.e. the proportions in this case) by country:


```{r eval=FALSE, include=FALSE}
evs.slim.gb %>% 
  group_by( "Trust in people" = as_label(v31),
            "Gender" = as_label(v225) ) %>% 
  summarise( "Age (mean)" = mean(age),
             "Age (std. dev.)" = sd(age),
             "Politics (mean)" = mean(v102),
             "Politics (std. dev.)" = sd(v102),
             n = n() )
```

Same grouping can be done with frq/desc ...
```{r}

```


```{r}

  
```

